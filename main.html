<html>

<head>
    <meta charset="utf-8" />
    <title>Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</title>

    <!-- Favicon references -->
    <link rel="icon" type="image/png" href="./images/viral_robot.png">
    <link rel="apple-touch-icon" href="./images/viral_robot.png">
    <link rel="icon" type="image/x-icon" href="favicon.ico">

    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        name="description" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="og:title" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="og:description" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="twitter:title" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="twitter:description" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?" rel="stylesheet" type="text/css" />

    <!-- üîé Added minimal CSS for click‚Äëto‚Äëzoom lightbox -->
    <style>
      /* make images clearly zoomable */
      img.zoomable { cursor: zoom-in; transition: transform .2s ease; }

      /* fullscreen overlay */
      .lightbox-overlay {
        position: fixed; inset: 0; display: none; align-items: center; justify-content: center;
        background: rgba(0,0,0,.9); z-index: 10000;
      }
      .lightbox-overlay.active { display: flex; }
      .lightbox-overlay img { max-width: 95vw; max-height: 95vh; box-shadow: 0 10px 40px rgba(0,0,0,.6); border-radius: 8px; }
      /* show zoom-out cursor while overlay is open */
      .lightbox-overlay, .lightbox-overlay * { cursor: zoom-out; }

      /* prevent background scroll when overlay is open */
      body.no-scroll { overflow: hidden; }
    </style>

    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#abstract">Overview</a></li>
                    <li><a href="#toy-experiments">Toy Experiments</a></li>
                    <li><a href="#CSFM">Methods</a></li>
                    <li><a href="#component-wise-analysis">Experiments</a></li>
                    <li><a href="#advantages-of-csfm">Advantages</a></li>
                    <li><a href="#target-representation-matters">Analysis</a></li>
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <div class="title-flex">
                    <div class="title-text-block">
                        <h1 class="title"><span class="gradient-text">Better Source Better Flow</span>: Learning Condition-Dependent<span class="title-break"></span>Source Distribution for Flow Matching</h1>
                        <h1 class="subtitle">arXiv 2026</h1>
                    </div>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://junwankimm.github.io/" target="_blank" class="author-text">
                        Junwan Kim<sup>1, *</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a target="_blank" class="author-text">
                        Jiho Park<sup>2, *</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://jeonseonghu.github.io/about-me" target="_blank" class="author-text">
                        Seonghu Jeon<sup>2</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://cvlab.kaist.ac.kr/" target="_blank" class="author-text">
                        Seungryong Kim<sup>2, ‚Ä†</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    <sup>1</sup>New York University &nbsp;&nbsp; <sup>2</sup>KAIST AI &nbsp;&nbsp
                    <br>
                    <sup>*</sup>Equal Contribution &nbsp;&nbsp; <sup>&dagger;</sup>Corresponding author
                </div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col"><a href="" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='' class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
            </div>

        </div>
    </div>

    <main class="main-content">
        <div class="container">
            <div class="tldr">
                <b>TL;DR</b>: VIRAL directly aligns MLLM's visual tokens with pretrained vision model features, preserving fine-grained visual details and boosting vision-centric reasoning beyond text-only supervision.
            </div>

            <div id="abstract" class="base-row section">
                <h2>Introducing the Condition-dependent Source Flow Matching (CSFM)</h2>
                <p class="paragraph">
                    <Strong>VIsual Representation ALignment (VIRAL)</Strong> is a simple but effective regularization strategy that aligns the internal visual representations of MLLMs with those from pretrained vision foundation models (VFMs). By explicitly supervising intermediate visual tokens with rich VFM features, VIRAL preserves semantically meaningful visual information that would otherwise be discarded under text-only learning.
                </p>
            </div>

            <div class="image-container">
                    <div class="image-content">
                        <img src="images/viral_teaser_hd.png" class="img large-image" alt="VIRAL teaser">
                    </div>

                    <p class="image-caption">We propose <strong>VIsual Representation ALignment (VIRAL)</strong> for multi-modal large language models (MLLMs), which introduces an auxiliary regularization objective on visual features to prevent MLLMs from discarding fine-grained visual attributes during training. VIRAL consistently produces more accurate visually grounded responses and yields substantial improvements over standard visual instruction tuning baselines when employing diverse vision towers, including CLIP and SigLIPv2.</p>
            </div>

            

            <section id="toy-experiments" class="section">
                <h2>Toy Experiments</h2>

                <p>MLLMs trained with text-only loss often discard fine-grained visual details. To address this, residual feature injection and explicit Visual Representation Alignment (VRA) loss are proposed to preserve richer visual semantics in intermediate layers.</p>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/comparsions_archs.png" class="img large-image" alt="Comparisons diagram">
                    </div>
                    <p class="image-caption"> Comparisons of (a) baseline visual instruction tuning, (b) re-injecting visual features via a residual connection, and (c) visual representation alignment. Both (b) and (c) are applied at the 16-th layer. </p>
                </div>

                <p>Visual features in MLLMs degrade with depth, but residual connections reduce this loss, and explicit alignment preserves them more effectively. Multimodal benchmarks show alignment loss yields the greatest gains, outperforming simple feature re-injection.</p>

                <div class="image-container">
                    <div class="image-content">
                        <img src="images/comparisons_results.png" class="img large-image" alt="Results chart">
                    </div>
                    <p class="image-caption">(d): Layer-wise alignment between visual tokens in MLLMs and vision encoder features measured by CKNNA, with shaded regions denoting middle layers that are especially important for visual understanding. (e): Benchmark performance corresponding to (a-c) </p>
                </div>
            </section>

            <section id="CSFM" class="section">
                <h2>Illustration of CSFM</h2>
                <div class="two-col">
                    <div class="col-left">
                        <p>
                        We freeze a pretrained Vision Foundation Model (VFM) $\mathcal{E}(\cdot)$ and treat its intermediate representation as a stable target $\mathbf{y} = \mathcal{E}(I)$. For each image $I$, the MLLM produces intermediate visual tokens $\mathbf{e}^\mathrm{img}_{\ell}$; a projection MLP $P_{\pi}(\cdot)$ maps them into the VFM feature space. 
                        </p>
                        <p>
                            We minimize a cosine-similarity based alignment loss $\mathcal{L}_{\mathrm{VRA}} = -\tfrac{1}{N}\sum_{i=1}^{N} \operatorname{sim}(\mathbf{y}_i, P_{\pi}(\mathbf{e}^{\mathrm{img}}_{i,\ell}))$, encouraging retention of fine-grained semantics that would otherwise collapse under pure instruction tuning. The overall objective combines texture generation loss $\mathcal{L}_{\mathrm{LM}}$ with the proposed VRA loss $\mathcal{L}_{\mathrm{VRA}}$, improving spatial reasoning and grounding without degrading language modeling quality.
                        </p>
                    </div>
                    <div class="col-right img">
                        <div class="image-container">
                            <img src="images/viral_architecture_hd.png" class="image-item img large-image z-depth-1" alt="VIRAL framework illustration">
                            <!-- <p class="image-caption">Building upon our findings in visual representation alignment, we align visual tokens from MLLM to strong, informative representations from VFMs.</p> -->
                        </div>
                    </div>
                </div>
            </section>

            <section id="component-wise-analysis" class="section">
                <h2>Component-wise Analysis of CSFM</h2>

                <p>Our method consistently boosts MLLM performance on vision-centric tasks by aligning intermediate visual features with a strong vision encoder. Gains persist even with SigLIPv2, showing that regularizing visual representations benefits MLLMs beyond compensating for contrastive pretraining limits.</p>
                <!-- <div class="image-container">
                    <img src="images/viral_alignment_table.png" class="img medium-image" alt="Alignment table">
                    <p class="image-caption">We report results of models trained with and without $\mathcal{L}_{\text{VRA}}$ across different vision encoders, evaluated on both vision-centric and general multimodal benchmarks. $\mathcal{L}_{\text{VRA}}$ consistently improves performance regardless of the encoder.</p>
                </div> -->


                <div class="table-container">
                    <table class="advantages-table">
                        <thead>
                            <tr>
                                <th class="model-col">Language Model</th>
                                <th class="model-col">Vision Encoder</th>
                                <th class="vra-col">VRA Loss</th>
                                <th>CV-Bench<sup>2D</sup></th>
                                <th>MMVP</th>
                                <th>What's Up</th>
                                <th>POPE</th>
                                <th>MMStar</th>
                                <th>MME</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="model-col" rowspan="4"><strong>Vicunna-7B-1.5</strong></td>
                                <td class="model-col" rowspan="2">CLIP</td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>56.82%</td>
                                <td>28.20%</td>
                                <td>40.13%</td>
                                <td>86.90%</td>
                                <td><strong>33.93%</strong></td>
                                <td>1650.21</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>59.67%</strong></td>
                                <td><strong>33.33%</strong></td>
                                <td><strong>48.55%</strong></td>
                                <td><strong>87.43%</strong></td>
                                <td><strong>33.93%</strong></td>
                                <td><strong>1694.52</strong></td>
                            </tr>
                            <tr>
                                <td class="model-col" rowspan="2">SigLipv2</td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>58.90%</td>
                                <td>28.22%</td>
                                <td>40.90%</td>
                                <td>90.13%</td>
                                <td>37.20%</td>
                                <td>1738.96</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>62.66%</strong></td>
                                <td><strong>33.11%</strong></td>
                                <td><strong>44.40%</strong></td>
                                <td><strong>90.77%</strong></td>
                                <td><strong>37.20%</strong></td>
                                <td><strong>1835.62</strong></td>
                            </tr>
                            <tr>
                                <td class="model-col" rowspan="4"><strong>Vicunna-13B-1.5</strong></td>
                                <td class="model-col" rowspan="2">CLIP</td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>57.51%</td>
                                <td>39.33%</td>
                                <td>44.44%</td>
                                <td>87.12%</td>
                                <td>34.47%</td>
                                <td>1599.04</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>58.97%</strong></td>
                                <td><strong>45.33%</strong></td>
                                <td><strong>62.26%</strong></td>
                                <td><strong>87.79%</strong></td>
                                <td><strong>37.99%</strong></td>
                                <td><strong>1636.62</strong></td>
                            </tr>
                            <tr>
                                <td class="model-col" rowspan="2">SigLipv2</td>
                                <td class="vra-col"><i class="fa fa-times vra-no" aria-label="VRA off"></i></td>
                                <td>58.90%</td>
                                <td>28.22%</td>
                                <td>40.90%</td>
                                <td>90.13%</td>
                                <td>37.20%</td>
                                <td>1738.96</td>
                            </tr>
                            <tr>
                                <td class="vra-col"><i class="fa fa-check vra-yes" aria-label="VRA on"></i></td>
                                <td><strong>62.66%</strong></td>
                                <td><strong>33.11%</strong></td>
                                <td><strong>44.40%</strong></td>
                                <td><strong>90.77%</strong></td>
                                <td><strong>37.20%</strong></td>
                                <td><strong>1835.62</strong></td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="image-caption">We report results of models trained with and without $\mathcal{L}_{\text{VRA}}$ across different vision encoders, evaluated on both vision-centric and general multimodal benchmarks. $\mathcal{L}_{\text{VRA}}$ consistently improves performance regardless of the encoder.</p>
                </div>

                <p>With VIRAL, performance on vision-centric tasks like counting and spatial reasoning improves significantly, outperforming LLaVA-7B-1.5 on challenging visual questions.</p>
                <div class="image-container">
                    <img src="images/viral_qual_hd.png" class="img large-image" alt="Qualitative examples">
                    <p class="image-caption">The left part presents PCA visualizations of intermediate representations, demonstrating that VIRAL yields more structured, semantically meaningful visual embeddings. The right part illustrates instance counting and spatial relation tasks, highlighting scenarios where VIRAL correctly answers questions while the baseline fails.</p>
                </div>
            </section>

            <section id="advantages-of-csfm" class="section">
                <h2>Advantages of CSFM</h2>

                <p>We identify optimal visual features and layers for aligning MLLM representations. We adopt DINOv2 as the default encoder. Layer-wise analysis shows the 16th layer yields the best results, with single-layer alignment outperforming multi-layer targets. Finally, we compare direct feature alignment with relation-based objectives over self-similarity matrices.</p>
                <div class="image-container">
                    <img src="images/viral_ablation_study.png" class="img medium-image" alt="Ablation study charts">
                    <p class="image-caption">We analyze the effects of (i) different vision foundation models (VFMs), (ii) alignment target layers, and (iii) alignment objectives, through evaluation on both vision-centric and general multimodal benchmarks. All experiments are conducted on the LLaVA-1.5-7B baseline.</p>
                </div>
            </section>

            <section id="target-representation-matters" class="section">
                <h2>Target Representation Matters for CSFM</h2>

                <p>We also provide various analyses to demonstrate the effectiveness of our method using the visual representation alignment loss.</p>
                
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide">
                            <img src="images/viral_attention_anlaysis.png"
                                class="comparison-slideshow-image" alt="Attention analysis">
                            <div class="image-caption">Qualitative comparison on text-to-image attention maps (left) and quantified spatial entropy of attention across layers and heads (right). Applying visual representation alignment encourages model to attend to more contextually important content, yielding a more focused and structured attention pattern.</div>
                        </div>
                        
                        <div class="slide">
                            <img src="images/viral_training_efficiency.png"
                                class="comparison-slideshow-image" alt="Training efficiency curves">
                            <div class="image-caption">Performance with and without $\mathcal{L}_\mathrm{VRA}$ evaluated every 1K steps on (a) POPE and (b) CV-Bench$^\mathrm{2D}$. Improved early-stage performance with $\mathcal{L}_\mathrm{VRA}$ indicates that VIRAL facilitates faster convergence.</div>
                        </div>

                        <div class="slide">
                            <img src="images/viral_permutation.png"
                                class="comparison-slideshow-image" alt="Permutation robustness bar chart">
                            <div class="image-caption">Number of correct predictions out of 788 spatial reasoning tasks in CV-Bench$^\mathrm{2D}$. Models with $\mathcal{L}_\mathrm{VRA}$ show larger performance drops under random permutation, indicating stronger sensitivity to spatial relationships.</div>
                        </div>
                        <div class="slide">
                            <img src="images/vfm_pca.png"
                                class="comparison-slideshow-image" alt="Permutation robustness bar chart">
                            <div class="image-caption"></div>
                        </div>
                    </div>

                    <button class="slideshow-nav prev" onclick="changeSlide(-1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(1)">‚ùØ</button>
                    
                    <div class="slideshow-controls">
                        <button class="play-pause" onclick="togglePlayPause()">
                            <i class="fa fa-pause"></i>
                        </button>
                    </div>
                </div>
            </section>
                
                

            <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p>In this work, we propose VIRAL, a simple yet effective training strategy that aligns the internal visual representations of MLLMs with those from powerful vision foundation models. Our approach preserves fine-grained visual semantics often discarded under text-only supervision, enabling more accurate spatial reasoning and object grounding. Extensive experiments across diverse benchmarks validate the effectiveness and generality of our method, demonstrating that visual representation alignment significantly enhances both performance and training efficiency in multimodal learning.</p>
            </section>

            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0"></pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>, <a href="https://describe-anything.github.io/" target="_blank">Describe-anything</a> and  <a href="https://cvlab-kaist.github.io/VIRAL/" target="_blank">VIRAL</a>.</p>
        </div>
    </footer>

    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

        // Initialize all slideshows
        document.querySelectorAll('.slideshow-container').forEach(container => {
            const slideshow = container.querySelector('.slideshow');
            const slides = slideshow.querySelectorAll('.slide');
            const prevButton = container.querySelector('.slideshow-nav.prev');
            const nextButton = container.querySelector('.slideshow-nav.next');
            const playPauseButton = container.querySelector('.play-pause');
            
            let currentSlide = 0;
            let autoplayInterval;
            let isPlaying = true;

            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                currentSlide = (n + slides.length) % slides.length;
                slides[currentSlide].classList.add('active');
            }

            function changeSlide(n) {
                showSlide(currentSlide + n);
                resetAutoplay();
            }

            function togglePlayPause() {
                if (isPlaying) {
                    clearInterval(autoplayInterval);
                    playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
                } else {
                    startAutoplay();
                    playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
                }
                isPlaying = !isPlaying;
            }

            function startAutoplay() {
                autoplayInterval = setInterval(() => {
                    showSlide(currentSlide + 1);
                }, 5000);
            }

            function resetAutoplay() {
                clearInterval(autoplayInterval);
                if (isPlaying) {
                    startAutoplay();
                }
            }

            // Initialize this slideshow
            showSlide(0);
            startAutoplay();

            // Add event listeners
            prevButton.addEventListener('click', () => changeSlide(-1));
            nextButton.addEventListener('click', () => changeSlide(1));
            playPauseButton.addEventListener('click', togglePlayPause);
        });

        // Handle main video play button
        const mainVideo = document.querySelector('.main-video');
        const playButton = document.querySelector('.play-button-overlay');
        
        if (mainVideo && playButton) {
            // Click play button to play video
            playButton.addEventListener('click', () => {
                mainVideo.play();
                mainVideo.classList.add('playing');
            });

            // Handle video play/pause events
            mainVideo.addEventListener('play', () => {
                mainVideo.classList.add('playing');
            });

            mainVideo.addEventListener('pause', () => {
                mainVideo.classList.remove('playing');
            });

            mainVideo.addEventListener('ended', () => {
                mainVideo.classList.remove('playing');
            });
        }

        // -----------------------------
        // üñºÔ∏è Click-to-zoom Lightbox
        // -----------------------------
        // Build overlay once
        const lightbox = document.createElement('div');
        lightbox.className = 'lightbox-overlay';
        lightbox.setAttribute('role', 'dialog');
        lightbox.setAttribute('aria-modal', 'true');
        lightbox.innerHTML = '<img alt="Expanded image">';
        document.body.appendChild(lightbox);
        const lightboxImg = lightbox.querySelector('img');

        function openLightbox(src, alt) {
            lightboxImg.src = src;
            lightboxImg.alt = alt || '';
            lightbox.classList.add('active');
            document.body.classList.add('no-scroll');
        }
        function closeLightbox() {
            lightbox.classList.remove('active');
            document.body.classList.remove('no-scroll');
            lightboxImg.src = '';
        }

        // Close on click anywhere or on Esc
        lightbox.addEventListener('click', closeLightbox);
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape' && lightbox.classList.contains('active')) closeLightbox();
        });

        // Mark target images as zoomable and wire up click
        const zoomableImages = document.querySelectorAll('.image-container img, .slideshow img, .main-content img.img, .hero-section img.img');
        zoomableImages.forEach(img => {
            img.classList.add('zoomable');
            img.addEventListener('click', () => {
                // support optional high-res source via data-fullsrc
                const src = img.getAttribute('data-fullsrc') || img.currentSrc || img.src;
                openLightbox(src, img.alt);
            });
        });
    });
    </script>
</body>
</html>