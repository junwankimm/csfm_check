<html>

<head>
    <meta charset="utf-8" />
    <title>Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</title>

    <!-- Favicon references -->
    <link rel="icon" type="image/x-icon" href="favicon.ico">

    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        name="description" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="og:title" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="og:description" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="twitter:title" />
    <meta
        content="Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching"
        property="twitter:description" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?" rel="stylesheet" type="text/css" />

    <!-- üîé Added minimal CSS for click‚Äëto‚Äëzoom lightbox -->
    <style>
      /* make images clearly zoomable */
      img.zoomable { cursor: zoom-in; transition: transform .2s ease; }

      /* fullscreen overlay */
      .lightbox-overlay {
        position: fixed; inset: 0; display: none; align-items: center; justify-content: center;
        background: rgba(0,0,0,.9); z-index: 10000;
      }
      .lightbox-overlay.active { display: flex; }
      .lightbox-overlay img { max-width: 95vw; max-height: 95vh; box-shadow: 0 10px 40px rgba(0,0,0,.6); border-radius: 8px; }
      /* show zoom-out cursor while overlay is open */
      .lightbox-overlay, .lightbox-overlay * { cursor: zoom-out; }

      /* prevent background scroll when overlay is open */
      body.no-scroll { overflow: hidden; }
    </style>

    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#abstract">Overview</a></li>
                    <li><a href="#CSFM">Methods</a></li>
                    <li><a href="#advantages-of-csfm">Advantages</a></li>
                    <li><a href="#component-wise-analysis">Analysis</a></li>
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <div class="title-flex">
                    <div class="title-text-block">
                        <h1 class="title"><span class="gradient-text">Better Source Better Flow</span>: Learning Condition-Dependent<span class="title-break"></span>Source Distribution for Flow Matching</h1>
                        <h1 class="subtitle">arXiv 2026</h1>
                    </div>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://junwankimm.github.io/" target="_blank" class="author-text">
                        Junwan Kim<sup>1, *</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a target="_blank" class="author-text">
                        Jiho Park<sup>2, *</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://jeonseonghu.github.io/about-me" target="_blank" class="author-text">
                        Seonghu Jeon<sup>2</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://cvlab.kaist.ac.kr/" target="_blank" class="author-text">
                        Seungryong Kim<sup>2, ‚Ä†</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    <sup>1</sup>New York University &nbsp;&nbsp; <sup>2</sup>KAIST AI &nbsp;&nbsp
                    <br>
                    <sup>*</sup>Equal Contribution &nbsp;&nbsp; <sup>&dagger;</sup>Corresponding author
                </div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col"><a href="" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='' class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
            </div>

        </div>
    </div>

    <main class="main-content">
        <div class="container">
            <div class="tldr">
                <b>Better sources lead to better flows:</b> learning condition-dependent source distributions, rather than a fixed Gaussian, improves conditional generation, such as text-to-image synthesis.
            </div>

            <div id="abstract" class="base-row section">
                <h2>Introducing the Condition-dependent Source Flow Matching (CSFM)</h2>
                <p class="paragraph">
                    While flow matching allows arbitrary source distributions, most existing approaches rely on a fixed Gaussian and rarely treat the source distribution itself as an optimization target. <Strong>Condition-dependent Source Flow Matching (CSFM)</Strong> demonstrates that a principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems, by learning a condition-dependent source distribution that better exploits rich conditioning information.
                </p>
            </div>

            <div class="image-container">
                    <div class="image-content">
                        <img src="images/main.png" class="img small-image" alt="Main teaser">
                    </div>

                    <p class="image-caption">Flow matching does not require the source distribution to be standard Gaussian. We leverage this flexibility by learning a condition-dependent source distribution, which reduces objective variance and improves flow matching performance.</p>
            </div>

            

            <section id="CSFM" class="section">
                <h2>CSFM</h2>
                <p>Using simple 2D toy experiments, we begin by studying what happens when the source distribution is learned naively. These experiments reveal clear failure modes highlighting that learning the source distribution is non-trivial and requires careful design.</p>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/toy_2d.png" class="img large-image" alt="Toy experiments">
                    </div>
                    <p class="image-caption"> We evaluate different source design choices on two 2D synthetic datasets with continuous conditions: <em>Eight Gaussians</em> (polar angle condition) and <em>Two Moons</em> (x-coordinate condition). We visualize transport trajectories, where <code>&times;</code> denotes source points (X<sub>0</sub>) and <code>&bull;</code> denotes generated target samples, with colors indicating the conditioning variable. </p>
                </div>
                <h3>Learning a Conditional Source Distribution (B)</h3>
                <p>
                We explicitly learn the source distribution in a condition-dependent manner, allowing it to adapt to the conditioning signal and enabling end-to-end optimization of the source‚Äìtarget coupling for conditional generation tasks such as text-to-image synthesis.
                </p>

                <h3>Conditional Gaussian for Sufficient Support (C) </h3>
                <p>
                However, implementing a conditional source as a deterministic mapping severely restricts support and leads to degenerate transport. To ensure sufficient support and smooth interpolation, we parameterize the source as a conditional Gaussian.
                </p>

                <h3>Limitations of KL-based Regularization (D) </h3>
                <p>
                A common approach to stabilizing conditional Gaussian sources is to regularize them toward a standard normal distribution using KL divergence. However, this constrains both the variance and the mean, preventing the source from relocating toward target modes and resulting in entangled transport paths with limited performance gains.
                </p>

                <h3>Variance-only Regularization (VarReg) (E) </h3>
                <p>
                To overcome this limitation, we introduce variance-only regularization, which controls the source variance while leaving the mean unconstrained. This allows the source distribution to move freely toward target modes while maintaining sufficient spread.
                </p>

                <h3>Source‚ÄìTarget Directional Alignment</h3>
                <p>
                In practical conditional generation tasks such as text-to-image synthesis, the flow model is typically equipped with strong conditional modeling capacity. Consequently, the flow matching objective provides relatively weak learning signals for the source distribution, making it harder to learn an informative source in practice. To address this, we explicitly encourage directional alignment between the learned source and target samples, guiding the source to better reflect the target structure in high-dimensional settings.
                </p>

                <p>
                Together, these components define <strong>Condition-dependent Source Flow Matching (CSFM)</strong>, a framework that learns condition-dependent source distributions with sufficient support, controlled variance, and explicit source‚Äìtarget alignment‚Äîresulting in reduced path entanglement and simplified flow learning.
                </p>
            </section>

            <section id="advantages-of-csfm" class="section">
                <h2>Advantages of CSFM</h2>

                <p>CSFM improves flow matching by reducing intrinsic variance through a condition-dependent source, leading to faster and more stable training. This results in higher sample quality with fewer training and sampling steps, including more robust few-step generation due to straighter transport paths. CSFM also consistently outperforms prior condition-aware source methods and remains effective when combined with guidance, demonstrating strong practicality in conditional generation settings.</p>
                
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide">
                            <img src="images/fmloss_gradvar.png"
                                class="comparison-slideshow-image" alt="Flow matching loss and gradient variance">
                            <div class="image-caption">CSFM shows a faster decrease in the flow matching loss and consistently lower gradient variance than standard FM, with particularly clear gains at small interpolation times near the source.</div>
                        </div>
                        
                        <div class="slide">
                            <img src="images/stepcomparison.png"
                                class="comparison-slideshow-image" alt="Training efficiency curves">
                            <div class="image-caption">The improved training dynamics translate into performance gains: CSFM converges faster in both FID and CLIP Score, including up to <strong>3.01√ó</strong> faster convergence in FID and <strong>2.48√ó</strong> faster convergence in CLIP Score.</div>
                        </div>

                        <div class="slide">
                            <img src="images/fewstep_fid_graph.png"
                                class="comparison-slideshow-image" alt="Few step FID comparison">
                            <div class="image-caption">CSFM degrades more gracefully as the number of sampling steps decreases, indicating reduced path variance and a straighter transport field. Under 1-Reflow, CSFM exhibits substantially smaller degradation under aggressive step reduction than standard FM.</div>
                        </div>
                        <div class="slide">
                            <img src="images/comparison.png"
                                class="comparison-slideshow-image" alt="Comparison with source learning methods">
                            <div class="image-caption">Compared to prior approaches that modify the training source distribution, such as CrossFlow and C<sup>2</sup>OT, CSFM achieves better FID and CLIP Score under the same evaluation setting.</div>
                        </div>
                        <div class="slide">
                            <img src="images/guidance.png"
                                class="comparison-slideshow-image" alt="Guidance comparison">
                            <div class="image-caption">Because the learned source already encodes conditional information, CSFM does not adopt classifier-free guidance and is instead evaluated with autoguidance. CSFM remains effective with autoguidance, achieving gains comparable to those in the no-guidance setting.</div>
                        </div>
                    </div>

                    <button class="slideshow-nav prev" onclick="changeSlide(-1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(1)">‚ùØ</button>
                    
                    <div class="slideshow-controls">
                        <button class="play-pause" onclick="togglePlayPause()">
                            <i class="fa fa-pause"></i>
                        </button>
                    </div>
                </div>

                <p>We scale CSFM to a 1.3B-parameter text-to-image model and find that learning a condition-dependent source remains effective at this scale. Qualitative results from the large model demonstrate that the benefits of learnable source distributions persist in high-capacity settings.</p>
                <div class="slideshow-container">
                    <div class="slideshow no-caption">
                        <div class="slide">
                            <img src="images/qual.png"
                                class="comparison-slideshow-image" alt="qualitative results">
                        </div>
                        
                        <div class="slide">
                            <img src="images/appx_qual1.png"
                                class="comparison-slideshow-image" alt="qualitative comparison results">
                        </div>

                        <div class="slide">
                            <img src="images/appx_qual2.png"
                                class="comparison-slideshow-image" alt="qualitative comparison results">
                        </div>
                    </div>

                    <button class="slideshow-nav prev" onclick="changeSlide(-1)">‚ùÆ</button>
                    <button class="slideshow-nav next" onclick="changeSlide(1)">‚ùØ</button>
                    
                    <div class="slideshow-controls">
                        <button class="play-pause" onclick="togglePlayPause()">
                            <i class="fa fa-pause"></i>
                        </button>
                    </div>
                </div>
                
            </section>


            <section id="component-wise-analysis" class="section">
                <h2>Component-wise Analysis of CSFM</h2>

                <p>We validate that the insights from toy experiments extend to practical text-to-image generation by constructing an ImageNet-based dataset with descriptive captions and evaluating our method on it. We examine key design choices in realistic settings and further test robustness across different conditioning architectures and text encoders. Across these settings, the proposed design remains consistently effective, demonstrating that our analysis generalizes beyond simplified toy scenarios to practical text-to-image models.</p>


                <div class="image-container">
                    <div class="image-content">
                        <img src="images/components.png" class="img large-image" alt="Component analysis">
                    </div>

                    <p class="image-caption">We evaluate individual components on a captioned ImageNet dataset. Gray rows indicate fixed-Gaussian baselines; bold entries denote the default setting; <sup>‚Ä†</sup> indicates a parameter-matched baseline.</p>
                </div>
            </section>


            <section id="target-representation-matters" class="section">
                <h2>Target Representation Matters</h2>
                <div class="two-col">
                    <div class="col-left">
                        <p>
                        The effectiveness of learning a condition-dependent source depends strongly on the structure of the target representation. When the target space exhibits well-separated and discriminative structure with respect to the conditioning signal, source learning becomes more effective.
                        </p>
                        
                        <p>
                        This effect is illustrated by t-SNE visualizations, where structured target representations (RAE) lead to more organized and discriminative learned sources, while poorly structured representations (SD-VAE) result in entangled targets and sources that resemble a fixed Gaussian prior. These observations highlight that CSFM benefits most when applied to target representations with clear condition-dependent structure.
                        </p>
                    </div>
                    <div class="col-right img">
                        <div class="image-container">
                            <img src="images/targetvssource.png" class="image-item img large-image z-depth-1" alt="Learned source distribution comparison">
                        </div>
                    </div>
                </div>
            </section>
                
                

            <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p>In this work, we present <Strong>Condition-dependent Source Flow Matching (CSFM)</Strong>, demonstrating that principled design of the source distribution can improve flow matching models by facilitating more favorable training dynamics and leading to consistent performance gains. Through extensive experiments and analyses, we elucidate the core mechanisms underlying our approach and show how condition-dependent source design enables more efficient and stable learning in complex conditional generation settings.
                </p>
            </section>

            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0"></pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>, <a href="https://describe-anything.github.io/" target="_blank">Describe-anything</a> and  <a href="https://cvlab-kaist.github.io/VIRAL/" target="_blank">VIRAL</a>.</p>
        </div>
    </footer>

    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

        // Initialize all slideshows
        document.querySelectorAll('.slideshow-container').forEach(container => {
            const slideshow = container.querySelector('.slideshow');
            const slides = slideshow.querySelectorAll('.slide');
            const prevButton = container.querySelector('.slideshow-nav.prev');
            const nextButton = container.querySelector('.slideshow-nav.next');
            const playPauseButton = container.querySelector('.play-pause');
            
            let currentSlide = 0;
            let autoplayInterval;
            let isPlaying = true;

            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                currentSlide = (n + slides.length) % slides.length;
                slides[currentSlide].classList.add('active');
            }

            function changeSlide(n) {
                showSlide(currentSlide + n);
                resetAutoplay();
            }

            function togglePlayPause() {
                if (isPlaying) {
                    clearInterval(autoplayInterval);
                    playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
                } else {
                    startAutoplay();
                    playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
                }
                isPlaying = !isPlaying;
            }

            function startAutoplay() {
                autoplayInterval = setInterval(() => {
                    showSlide(currentSlide + 1);
                }, 5000);
            }

            function resetAutoplay() {
                clearInterval(autoplayInterval);
                if (isPlaying) {
                    startAutoplay();
                }
            }

            // Initialize this slideshow
            showSlide(0);
            startAutoplay();

            // Add event listeners
            prevButton.addEventListener('click', () => changeSlide(-1));
            nextButton.addEventListener('click', () => changeSlide(1));
            playPauseButton.addEventListener('click', togglePlayPause);
        });

        // Handle main video play button
        const mainVideo = document.querySelector('.main-video');
        const playButton = document.querySelector('.play-button-overlay');
        
        if (mainVideo && playButton) {
            // Click play button to play video
            playButton.addEventListener('click', () => {
                mainVideo.play();
                mainVideo.classList.add('playing');
            });

            // Handle video play/pause events
            mainVideo.addEventListener('play', () => {
                mainVideo.classList.add('playing');
            });

            mainVideo.addEventListener('pause', () => {
                mainVideo.classList.remove('playing');
            });

            mainVideo.addEventListener('ended', () => {
                mainVideo.classList.remove('playing');
            });
        }

        // -----------------------------
        // üñºÔ∏è Click-to-zoom Lightbox
        // -----------------------------
        // Build overlay once
        const lightbox = document.createElement('div');
        lightbox.className = 'lightbox-overlay';
        lightbox.setAttribute('role', 'dialog');
        lightbox.setAttribute('aria-modal', 'true');
        lightbox.innerHTML = '<img alt="Expanded image">';
        document.body.appendChild(lightbox);
        const lightboxImg = lightbox.querySelector('img');

        function openLightbox(src, alt) {
            lightboxImg.src = src;
            lightboxImg.alt = alt || '';
            lightbox.classList.add('active');
            document.body.classList.add('no-scroll');
        }
        function closeLightbox() {
            lightbox.classList.remove('active');
            document.body.classList.remove('no-scroll');
            lightboxImg.src = '';
        }

        // Close on click anywhere or on Esc
        lightbox.addEventListener('click', closeLightbox);
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape' && lightbox.classList.contains('active')) closeLightbox();
        });

        // Mark target images as zoomable and wire up click
        const zoomableImages = document.querySelectorAll('.image-container img, .slideshow img, .main-content img.img, .hero-section img.img');
        zoomableImages.forEach(img => {
            img.classList.add('zoomable');
            img.addEventListener('click', () => {
                // support optional high-res source via data-fullsrc
                const src = img.getAttribute('data-fullsrc') || img.currentSrc || img.src;
                openLightbox(src, img.alt);
            });
        });
    });
    </script>
</body>
</html>